# scikit-learn



MLP trains using Stochastic Gradient Descent, Adam, or L-BFGS. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.

$w \leftarrow w-\eta(\frac {dR(w)}{dw} + \frac {dLoss}{dw}) $


